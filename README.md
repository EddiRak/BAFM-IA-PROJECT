# Projet de Traitement ETL des CDR/EDR

Ce projet implÃ©mente un pipeline ETL (Extract, Transform, Load) pour traiter diffÃ©rents types de fichiers CDR (Call Detail Records) et EDR (Event Detail Records) provenant de diffÃ©rents systÃ¨mes de tÃ©lÃ©communication.

## ðŸ“‹ Table des matiÃ¨res

1. [Vue d'ensemble](#vue-densemble)
2. [Structure du projet](#structure-du-projet)
3. [Types de donnÃ©es traitÃ©s](#types-de-donnÃ©es-traitÃ©s)
4. [Configuration requise](#configuration-requise)
5. [Installation](#installation)
6. [Utilisation](#utilisation)
7. [DÃ©tails techniques](#dÃ©tails-techniques)
8. [Validation des donnÃ©es](#validation-des-donnÃ©es)

## Vue d'ensemble

Le pipeline ETL traite les donnÃ©es de plusieurs systÃ¨mes :
- AIR (Account Information Records)
- MSC (Mobile Switching Center)
- SDP (Service Delivery Platform)
- SGSN (Serving GPRS Support Node)
- OCC (Online Charging Center)
- CCN (Call Control Node)

Chaque systÃ¨me gÃ©nÃ¨re des fichiers dans un format spÃ©cifique qui sont transformÃ©s en fichiers Parquet pour une analyse ultÃ©rieure.

## Structure du projet

```
CDR_EDR_unzipped/
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ src/
â”‚   â””â”€â”€ etl/
â”‚       â”œâ”€â”€ modules              # modules utilisÃ©s
â”‚       â”œâ”€â”€ main_etl.py          # Point d'entrÃ©e principal
â”‚       â”œâ”€â”€ air_etl.py           # Traitement AIR
â”‚       â”œâ”€â”€ msc_etl.py           # Traitement MSC
â”‚       â”œâ”€â”€ sdp_etl.py           # Traitement SDP
â”‚       â”œâ”€â”€ sgsn_etl.py          # Traitement SGSN
â”‚       â”œâ”€â”€ occ_etl.py           # Traitement OCC
â”‚       â””â”€â”€ ccn_etl.py           # Traitement CCN
â”œâ”€â”€ raw_data/                    # DonnÃ©es brutes
â”‚   â”œâ”€â”€ AIR/
â”‚   â”œâ”€â”€ MSC/
â”‚   â”œâ”€â”€ SDP/
â”‚   â”œâ”€â”€ SGSN/
â”‚   â”œâ”€â”€ OCC/
â”‚   â””â”€â”€ CCN/
â””â”€â”€ processed_data/              # DonnÃ©es transformÃ©es
    â””â”€â”€ ETL/
        â”œâ”€â”€ AIR/
        â”œâ”€â”€ MSC/
        â”œâ”€â”€ SDP/
        â”œâ”€â”€ SGSN/
        â”œâ”€â”€ OCC/
        â””â”€â”€ CCN/
```

## Types de donnÃ©es traitÃ©s

### AIR
- **Format d'entrÃ©e** : Fichiers .AIR ou .AIR.gz
- **Types d'enregistrements** : 
  - ADJUSTMENT (adjustmentRecordV2)
  - REFILL (refillRecordV2)

### MSC
- **Format d'entrÃ©e** : Fichiers TTFILE
- **Services traitÃ©s** :
  - mSOriginatingSMSinMSC
  - mSTerminatingSMSinMSC
  - mSTerminating
  - sSProcedure
  - transit
  - mSOriginating
  - callForwarding

### SDP
- **Format d'entrÃ©e** : Fichiers .ASN ou .ASN.gz
- **Types d'enregistrements** :
  - ACCOUNT_ADJUSTMENT
  - PERIODIC_ACCOUNT_MGMT
  - LIFE_CYCLE_CHANGE

### SGSN
- **Format d'entrÃ©e** : Fichiers MGANPGW
- **SÃ©parateur** : |
- **Format de sortie** : Un fichier Parquet unique

### OCC
- **Format d'entrÃ©e** : Fichiers .ber, .ber_NP (DATA) et .ber-SMS (SMS)
- **SÃ©parateur** : ,
- **Sorties distinctes** : 
  - DATA (occ_data.parquet)
  - SMS (occ_sms.parquet)

### CCN
- **Format d'entrÃ©e** : Fichiers CCNCDR
- **SÃ©parateur** : ,
- **Format de sortie** : Un fichier Parquet unique

## Configuration requise

- Python 3.8+
- pandas
- dask
- pyarrow (pour le format Parquet)
- gzip (pour la dÃ©compression)

## Installation

1. Cloner le repository :
```bash
git clone https://github.com/EddiRak/BAFM-IA-PROJECT.git
cd CDR_EDR_unzipped
```

2. Installer les dÃ©pendances :
```bash
pip install -r requirements.txt
```

## Utilisation

1. Placer les fichiers source dans les dossiers appropriÃ©s sous `raw_data/`

2. ExÃ©cuter le pipeline complet :
```bash
python src/etl/main_etl.py
```

Ou exÃ©cuter un module spÃ©cifique :
```bash
python src/etl/air_etl.py  # Pour AIR uniquement
python src/etl/msc_etl.py  # Pour MSC uniquement
# etc.
```

## DÃ©tails techniques

### Traitement parallÃ¨le
- Utilisation de Dask pour le traitement parallÃ¨le
- Nombre de workers configurable (par dÃ©faut : 4)
- Variable d'environnement : DASK_WORKERS

### Gestion des fichiers
- Support des fichiers compressÃ©s (.gz)
- DÃ©tection automatique du format
- Gestion robuste des erreurs

### Logging
Format unifiÃ© pour tous les modules :
```
INFO     - ETL XXX - DÃ©marrage
INFO     - â€¢ Date      : YYYY-MM-DD HH:MM:SS
INFO     - â€¢ Source    : <dossier_source>
INFO     - â€¢ Cible     : <dossier_cible>
...
INFO     - ======================================
INFO     - RÃ©sultats du traitement:
INFO     - â€¢ Fichiers traitÃ©s avec succÃ¨s : X
INFO     - â€¢ Fichiers en erreur          : Y
INFO     - â€¢ Total des enregistrements   : Z
INFO     - â€¢ Temps d'exÃ©cution           : T.tt secondes
INFO     - ======================================
```

## Validation des donnÃ©es

Pour vÃ©rifier les donnÃ©es transformÃ©es, utiliser le notebook `verification.ipynb` qui permet de :
- Visualiser les donnÃ©es de chaque systÃ¨me
- VÃ©rifier les nombres d'enregistrements
- Examiner la structure des donnÃ©es
- Valider les transformations

## Variables d'environnement

Les chemins peuvent Ãªtre configurÃ©s via des variables d'environnement :
- RAW_DATA_PATH
- PROCESSED_DATA_PATH
- DASK_WORKERS

Par dÃ©faut, les chemins relatifs sont utilisÃ©s si les variables ne sont pas dÃ©finies.
